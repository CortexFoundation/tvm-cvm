{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "def parse_cfg(cfgfile):\n",
    "    def erase_comment(line):\n",
    "        line = line.split('#')[0]\n",
    "        return line\n",
    "    blocks = []\n",
    "    fp = open(cfgfile, 'r')\n",
    "    block =  None\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        line = line.rstrip()\n",
    "        if line == '' or line[0] == '#':\n",
    "            line = fp.readline()\n",
    "            continue        \n",
    "        elif line[0] == '[':\n",
    "            if block:\n",
    "                blocks.append(block)\n",
    "            block = OrderedDict()\n",
    "            block['type'] = line.lstrip('[').rstrip(']')\n",
    "            # set default value\n",
    "            if block['type'] == 'convolutional':\n",
    "                block['batch_normalize'] = 0\n",
    "        else:\n",
    "            line = erase_comment(line)\n",
    "            key,value = line.split('=')\n",
    "            key = key.strip()\n",
    "            if key == 'type':\n",
    "                key = '_type'\n",
    "            value = value.strip()\n",
    "            block[key] = value\n",
    "        line = fp.readline()\n",
    "\n",
    "    if block:\n",
    "        blocks.append(block)\n",
    "    fp.close()\n",
    "    return blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a', 'c', 'b'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg_file = \"/home/serving/MXNet-YOLO/darknet2mxnet/pytorch-caffe-darknet-convert/yolov3-tiny.cfg\"\n",
    "weight_file = \"/home/serving/MXNet-YOLO/darknet2mxnet/pytorch-caffe-darknet-convert/yolov3-tiny.weights\"\n",
    "\n",
    "\n",
    "a = set()\n",
    "a.add(\"a\")\n",
    "a.add(\"b\")\n",
    "a.add(\"c\")\n",
    "a.add(\"a\")\n",
    "print (a)\n",
    "a.remove(\"a\")\n",
    "len(a)\n",
    "\n",
    "b = dict()\n",
    "b['a'] = 'c'\n",
    "b.clear()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "\n",
    "def darknet2mxnet(cfgfile, weightfile, prefix):\n",
    "    blocks = parse_cfg(cfgfile)\n",
    "\n",
    "    num_input = []\n",
    "    fp = open(weightfile, 'rb')\n",
    "    # header = np.fromfile(fp, count=4, dtype=np.int32)\n",
    "    header = np.fromfile(fp, count=3, dtype=np.int32)\n",
    "    seen = np.fromfile(fp, count=1, dtype=np.int64)\n",
    "    buf = np.fromfile(fp, dtype=np.float32)\n",
    "    fp.close()\n",
    "\n",
    "    print (buf.shape[0])\n",
    "    start = 0\n",
    "    layer_id = 1  # track conv layer\n",
    "    arg_params = dict()\n",
    "    aux_params = dict()\n",
    "    total_layer_id = 1\n",
    "\n",
    "    for block in blocks:\n",
    "        if block['type'] == 'net':\n",
    "            num_input.append(int(block['channels']))\n",
    "        elif block['type'] == 'convolutional': # name, type, param(#input, #output, kernel, pad, stride)\n",
    "            conv_layer = OrderedDict()\n",
    "            if 'name' in block:\n",
    "                conv_layer['name'] = block['name']\n",
    "            else:\n",
    "                conv_layer['name'] = 'layer%d-conv' % layer_id\n",
    "            convolution_param = OrderedDict()\n",
    "            convolution_param['num_input'] = num_input[-1]\n",
    "            convolution_param['num_output'] = int(block['filters'])\n",
    "            # update #input\n",
    "            num_input.append(int(block['filters']))\n",
    "            convolution_param['kernel_size'] = int(block['size'])\n",
    "            if block['batch_normalize'] == '1':\n",
    "                convolution_param['bias_term'] = 'false'\n",
    "            else:\n",
    "                convolution_param['bias_term'] = 'true'\n",
    "            conv_layer['convolution_param'] = convolution_param\n",
    "\n",
    "            # print 'conv%d' % total_layer_id, num_input[-2], num_input[-1]\n",
    "\n",
    "            conv_weight_name = conv_layer['name'] + '_weight'\n",
    "            conv_bias_name = conv_layer['name'] + '_bias'\n",
    "            arg_params[conv_weight_name] = np.zeros((conv_layer['convolution_param']['num_output'],\n",
    "                                                     conv_layer['convolution_param']['num_input'],\n",
    "                                                     conv_layer['convolution_param']['kernel_size'],\n",
    "                                                     conv_layer['convolution_param']['kernel_size']))\n",
    "\n",
    "            print (conv_weight_name, arg_params[conv_weight_name].shape)\n",
    "\n",
    "            if block['batch_normalize'] == '1':\n",
    "                bn_layer = OrderedDict()\n",
    "                if 'name' in block:\n",
    "                    bn_layer['name'] = '%s-bn' % block['name']\n",
    "                else:\n",
    "                    bn_layer['name'] = 'layer%d-bn' % layer_id\n",
    "\n",
    "                bn_beta = bn_layer['name'] + '_beta'\n",
    "                bn_gamma = bn_layer['name'] + '_gamma'\n",
    "                bn_avg = bn_layer['name'] + '_moving_mean'\n",
    "                bn_var = bn_layer['name'] + '_moving_var'\n",
    "                arg_params[bn_beta] = np.zeros((conv_layer['convolution_param']['num_output']))\n",
    "                arg_params[bn_gamma] = np.zeros((conv_layer['convolution_param']['num_output']))\n",
    "                aux_params[bn_avg] = np.zeros((conv_layer['convolution_param']['num_output']))\n",
    "                aux_params[bn_var] = np.zeros((conv_layer['convolution_param']['num_output']))\n",
    "\n",
    "                print (bn_gamma, arg_params[bn_gamma].shape)\n",
    "\n",
    "                start = load_conv_bn2caffe(buf, start, arg_params, aux_params, conv_weight_name,\n",
    "                                           bn_beta, bn_gamma, bn_avg, bn_var)\n",
    "            else:\n",
    "                arg_params[conv_bias_name] = np.zeros((conv_layer['convolution_param']['num_output']))\n",
    "                print (conv_bias_name, arg_params[conv_bias_name].shape)\n",
    "                start = load_conv2caffe(buf, start, arg_params, conv_weight_name, conv_bias_name)\n",
    "\n",
    "            layer_id = layer_id+1\n",
    "            total_layer_id = total_layer_id + 1\n",
    "\n",
    "        elif block['type'] == 'route':\n",
    "            sub_idx = map(int, block['layers'].split(','))\n",
    "            get_output = 0\n",
    "            # print 'route%d' % total_layer_id\n",
    "            for i in sub_idx:\n",
    "                # print i, num_input[i]\n",
    "                get_output = get_output + num_input[i]\n",
    "            num_input.append(get_output)\n",
    "            # print 'output', num_input[-1]\n",
    "            total_layer_id = total_layer_id + 1\n",
    "\n",
    "        elif block['type'] == 'reorg':\n",
    "            stride = int(block['stride'])\n",
    "            num_input.append(num_input[-1]*stride*stride)\n",
    "            # print 'reorg%d' % total_layer_id, num_input[-2], num_input[-1]\n",
    "            total_layer_id = total_layer_id + 1\n",
    "\n",
    "        elif block['type'] == 'maxpool':\n",
    "            num_input.append(num_input[-1])\n",
    "            # print 'max%d' % total_layer_id, num_input[-2], num_input[-1]\n",
    "            total_layer_id = total_layer_id + 1\n",
    "\n",
    "    print (num_input)\n",
    "    print (start)\n",
    "    save_checkpoint(prefix, 0, arg_params, aux_params)\n",
    "\n",
    "def load_conv_bn2caffe(buf, start, arg_params ,aux_params, conv_weight_name, bn_beta, bn_gamma, bn_avg, bn_var):\n",
    "    conv_weight = arg_params[conv_weight_name]\n",
    "    running_mean = aux_params[bn_avg]\n",
    "    running_var = aux_params[bn_var]\n",
    "    scale_weight = arg_params[bn_gamma]\n",
    "    scale_bias = arg_params[bn_beta]\n",
    "\n",
    "    arg_params[bn_beta] = mx.nd.array(np.reshape(buf[start:start+scale_bias.size], scale_bias.shape))\n",
    "    start = start + scale_bias.size\n",
    "    arg_params[bn_gamma] = mx.nd.array(np.reshape(buf[start:start+scale_weight.size], scale_weight.shape))\n",
    "    start = start + scale_weight.size\n",
    "    aux_params[bn_avg] = mx.nd.array(np.reshape(buf[start:start+running_mean.size], running_mean.shape))\n",
    "    start = start + running_mean.size\n",
    "    aux_params[bn_var] = mx.nd.array(np.reshape(buf[start:start+running_var.size], running_var.shape))\n",
    "    start = start + running_var.size\n",
    "    # bn_param[2].data[...] = np.array([1.0])\n",
    "    # convolution weight\n",
    "    arg_params[conv_weight_name] = mx.nd.array(np.reshape(buf[start:start+conv_weight.size], conv_weight.shape))\n",
    "    start = start + conv_weight.size\n",
    "    return start\n",
    "\n",
    "def load_conv2caffe(buf, start, arg_params, conv_weight_name, conv_bias_name):\n",
    "    weight = arg_params[conv_weight_name]\n",
    "    bias = arg_params[conv_bias_name]\n",
    "    arg_params[conv_bias_name] = mx.nd.array(np.reshape(buf[start:start+bias.size], bias.shape))\n",
    "    start = start + bias.size\n",
    "    arg_params[conv_weight_name] = mx.nd.array(np.reshape(buf[start:start+weight.size], weight.shape))\n",
    "    start = start + weight.size\n",
    "    return start\n",
    "\n",
    "def save_checkpoint(prefix, epoch, arg_params, aux_params):\n",
    "    save_dict = {('arg:%s' % k) : v for k, v in arg_params.items()}\n",
    "    save_dict.update({('aux:%s' % k) : v for k, v in aux_params.items()})\n",
    "    param_name = '%s-%04d.params' % (prefix, epoch)\n",
    "    mx.nd.save(param_name, save_dict)\n",
    "\n",
    "def load(*args):\n",
    "    if len(args) != 3:\n",
    "        print('Usage:')\n",
    "        print('python darknet2mxnet.py darknet.cfg darknet.weights mxnet.params')\n",
    "        print('')\n",
    "        print('please add name field for each block to avoid generated name')\n",
    "        exit()\n",
    "\n",
    "    cfgfile = args[0]\n",
    "    weightfile = args[1]\n",
    "    prefix = args[2]\n",
    "    darknet2mxnet(cfgfile, weightfile, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8858734\n",
      "layer1-conv_weight (16, 3, 3, 3)\n",
      "layer1-bn_gamma (16,)\n",
      "layer2-conv_weight (32, 16, 3, 3)\n",
      "layer2-bn_gamma (32,)\n",
      "layer3-conv_weight (64, 32, 3, 3)\n",
      "layer3-bn_gamma (64,)\n",
      "layer4-conv_weight (128, 64, 3, 3)\n",
      "layer4-bn_gamma (128,)\n",
      "layer5-conv_weight (256, 128, 3, 3)\n",
      "layer5-bn_gamma (256,)\n",
      "layer6-conv_weight (512, 256, 3, 3)\n",
      "layer6-bn_gamma (512,)\n",
      "layer7-conv_weight (1024, 512, 3, 3)\n",
      "layer7-bn_gamma (1024,)\n",
      "layer8-conv_weight (256, 1024, 1, 1)\n",
      "layer8-bn_gamma (256,)\n",
      "layer9-conv_weight (512, 256, 3, 3)\n",
      "layer9-bn_gamma (512,)\n",
      "layer10-conv_weight (255, 512, 1, 1)\n",
      "layer10-conv_bias (255,)\n",
      "layer11-conv_weight (128, 1024, 1, 1)\n",
      "layer11-bn_gamma (128,)\n",
      "layer12-conv_weight (256, 256, 3, 3)\n",
      "layer12-bn_gamma (256,)\n",
      "layer13-conv_weight (255, 256, 1, 1)\n",
      "layer13-conv_bias (255,)\n",
      "[3, 16, 16, 32, 32, 64, 64, 128, 128, 256, 256, 512, 512, 1024, 256, 512, 255, 1024, 128, 256, 256, 255]\n",
      "8662126\n"
     ]
    }
   ],
   "source": [
    "load(cfg_file, weight_file, \"./yolo-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import ndarrray as nd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
